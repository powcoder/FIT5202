{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lcWYqAxbFIee"
   },
   "source": [
    "# FIT5202 Data processing for Big data\n",
    "\n",
    "##  Activity: Parallel Aggregation\n",
    "\n",
    "For this tutorial we will implement different operations and aggregations like distinct, group by and order by on Spark DataFrames. In the second part, you will need to use all these operations to answer the lab tasks.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [SparkContext and SparkSession](#one)\n",
    "* [Parallel Aggregation](#two)\n",
    "    * [Group By](#groupby)        \n",
    "    * [Sort By](#sortby)    \n",
    "    * [Distinct](#distinct)    \n",
    "* [Miscellaneous DataFrame Operations](#misc)\n",
    "    * [Describe a column](#describe_column)\n",
    "    * [Adding/Dropping Columns](#add_drop_column)    \n",
    "    * [PySpark Built-in Functions](#pyspark_functions)       \n",
    "    * [User Defined Functions : UDFs](#udf) \n",
    "* [Lab Tasks](#lab-task-1)\n",
    "    * [Lab Task 1](#lab-task-1)\n",
    "    * [Lab Task 2](#lab-task-2)\n",
    "    * [Lab Task 3](#lab-task-3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iChz1a-tk7aP"
   },
   "source": [
    "<a class=\"anchor\" name=\"one\"></a>\n",
    "## Import Spark classes and create Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>In the cell block below, \n",
    "<ul>\n",
    "    <li>Create a SparkConfig object with application name set as \"Parallel Aggregation\"</li>\n",
    "    <li>specify 2 cores for processing</li>\n",
    "    <li>Use the configuration object to create a spark session named as <strong>spark</strong>.</li>\n",
    "    </ul>\n",
    "    \n",
    "<p><strong style=\"color:red\">Important:</strong> You cannot proceed to other steps without completing this.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CsQiS58Ak7aQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Import libraries needed from pyspark\n",
    "\n",
    "# TODO: Create Spark Configuration Object\n",
    "\n",
    "# TODO: Create SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "783QFsKyk7aV"
   },
   "source": [
    "<a class=\"anchor\" name=\"two\"></a>\n",
    "## Parallel Aggregation\n",
    "\n",
    "Now we will implement basic aggregation functionalities and visualise the parallelism embedded in Spark as well as the execution plan and functions done to perform these kind of queries.\n",
    "\n",
    "In this tutorial, you will use two csv files as datasets which contains information about all the athletes that have participated in the Summer and Winter Olympics (athlete_events.csv) as well as the information of their countries (noc_regions.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PM_c05Ck7aW",
    "outputId": "601ea949-37f3-45a8-f64f-f080af2a90da"
   },
   "outputs": [],
   "source": [
    "# Read athlete events data as dataframe\n",
    "df_events = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('athlete_events.csv')\n",
    "\n",
    "# Create Views from Dataframes\n",
    "df_events.createOrReplaceTempView(\"sql_events\")\n",
    "\n",
    "## Verifying the number of partitions for each dataframe\n",
    "## You can explore the data of each csv file with the function printSchema()\n",
    "print(f\"####### DICTIONARY INFO:\")\n",
    "print(f\"Number of partitions: {df_events.rdd.getNumPartitions()}\")\n",
    "df_events.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fytB-sHek7ad"
   },
   "source": [
    "### Group By <a class=\"anchor\" name=\"groupby\"></a>\n",
    "This part contains a simple aggregation query. Look into the query plan and level of parallelism in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqM6wRhik7ae"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#### Aggregate the dataset by 'Year' and count the total number of athletes using Dataframe\n",
    "agg_attribute = 'Year'\n",
    "df_count = df_events.groupby(agg_attribute).agg(F.count(agg_attribute).alias('Total'))\n",
    "\n",
    "#### Aggregate the dataset by 'Year' and count the total number of athletes using SQL\n",
    "sql_count = spark.sql('''\n",
    "  SELECT year,count(*)\n",
    "  FROM sql_events\n",
    "  GROUP BY year\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0lO3gQ-k7ai",
    "outputId": "41efc5f2-6e28-40e2-8388-2d6906756b91"
   },
   "outputs": [],
   "source": [
    "df_count.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "  The same thing can be done using \n",
    "    <code>groupby(agg_attribute).agg({'Year':'count'})</code>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort By <a class=\"anchor\" name=\"sortby\"></a>\n",
    "We can use orderBy operation to sort the dataframe based on some column.\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "    You can specify the sort order using the method <strong>desc()</strong>\n",
    "    <code>orderBy(df_events.Year.desc())</code>    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.select('Year','Name','Team').orderBy(df_events.Year).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INsF2yY_k7an"
   },
   "source": [
    "### Distinct <a class=\"anchor\" name=\"distinct\"></a>\n",
    "This part contains a simple query to get the distinct values of one of the attributes and then sorting them by the same attribute in ascending order.\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">NOTE: </strong>\n",
    "    We can use <code>.sort()</code> method to do the sorting as well. In the second parameter of the method, we can specify the order of the sorting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUMVzlrkk7ar"
   },
   "outputs": [],
   "source": [
    "#### Get the distinct values for 'Year' in the dataset using Dataframe\n",
    "df_distinct_sort = df_events.select('Year').distinct().sort('Year', ascending=True)\n",
    "\n",
    "#### Get the distinct values for 'Year' in the dataset using SQL\n",
    "sql_distinct_sort = spark.sql('''\n",
    "  SELECT distinct Year\n",
    "  FROM sql_events\n",
    "  ORDER BY year\n",
    "''')\n",
    "df_distinct_sort.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gNRFFWwk7au",
    "outputId": "6ef467dd-ee39-40c6-efee-157d48a52a5a"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong>Sort the above dataframe i.e. events by <strong>Year</strong> in descending order.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" name=\"misc\"></a>\n",
    "## Miscellaneous Dataframe Operations\n",
    "These are the examples of other dataframe operations which are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing a Column <a class=\"anchor\" name=\"describe_column\"></a>\n",
    "The <code>describe()</code> melthod gives the statistical summary of the column. If the column is not specified, it gives the summary of the whole dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events.describe('Team').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Dropping a column in dataframe <a class=\"anchor\" name=\"add_drop_column\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is an example of adding a new column based on the previous column\n",
    "df_events_new = df_events.withColumn('Years Ago',2020-df_events.Year).select('Years Ago','Name')\n",
    "display(df_events_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>\n",
    "    You can use the <code>.drop('column_name')</code> method to drop columns from a dataframe. Try this method and drop the column created above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PySpark Functions <a class=\"anchor\" name=\"pyspark_functions\"></a>\n",
    "You can use PySpark built-in functions along with the <code>withColumn()</code> API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#Changing the datatype \n",
    "#using the display method to see the columns and datatypes of a dataframe\n",
    "display(df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use CAST to change the datatype of Age Column \n",
    "df_events = df_events.withColumn('Age',F.col('Age').cast(IntegerType()))\n",
    "display(df_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The folowing example uses another inbuilt function to extract year from the Games column\n",
    "df_events = df_events.withColumn('Games Year',F.split(df_events.Games,' ')[0])\n",
    "df_events.select('Games Year').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame UDFs (User Defined Functions) <a class=\"anchor\" name=\"udf\"></a>\n",
    "Similar to map operation in an RDDs, sometimes we might want to apply a complex operation to the DataFrame, something which is not provided by the DataFrame APIs. In such scenarios, using a Spark UDF could be handy. To use Spark UDFs, we need to use the F.udf to convert a regular function to a Spark UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, the following function does the same things as the above built-function but this time we are using a udf\n",
    "#1. The function is defined\n",
    "def extract_year(s):\n",
    "    return int(s.split(' ')[0])\n",
    "\n",
    "#2. Calling the UDF with DataFrame\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "#First Register the function as UDF\n",
    "extract_year_udf = udf(extract_year,IntegerType())\n",
    "\n",
    "#Call the function\n",
    "df_events.select('Games',extract_year_udf('Games').alias(\"Game Year\")).show(5)\n",
    "\n",
    "#4. Calling with Spark SQL\n",
    "#First Register the function as UDF\n",
    "spark.udf.register('extract_year',extract_year,IntegerType())\n",
    "\n",
    "#Call the function \n",
    "df_events.createOrReplaceTempView('events')\n",
    "df_sql = spark.sql('''select Games, extract_year(Games) as Game_Year from events''')\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FrgOeLck7a9"
   },
   "source": [
    "## Combining DataFrame operations <a class=\"anchor\" name=\"combine\"></a>\n",
    "Now that we have used the main SQL operations to process data, you will implement several queries using Spark Dataframes and SQL to solve each of the queries.\n",
    "The dataset used for this section will be the 2 attached csv files:\n",
    "* <code>athlete_events.csv</code>\n",
    "* <code>noc_regions.csv</code>\n",
    "\n",
    "The first dataset was already used in the first part of this tutorial. The second one contains the countries with some additional information\n",
    "In this section, you will need to complete most of the code but in some parts, a hint or the name of variables will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMgH70wik7a9"
   },
   "outputs": [],
   "source": [
    "# Stop the previous Spark Context to clean all the previous executions from the previous section\n",
    "sc.stop()\n",
    "# Verify that the Spark UI is not running anymore or that there is no content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mh-9XqEIk7bB"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>\n",
    "Since we have removed the Spark Context in the previous code block, start the context once again by using the SparkSession object in the next code block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptcMBponk7bC"
   },
   "outputs": [],
   "source": [
    "# TODO: Import libraries needed from pyspark\n",
    "\n",
    "# TODO: Create Spark Configuration Object\n",
    "\n",
    "# TODO: Create SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PChZJoTk7bE"
   },
   "source": [
    "### Create Spark data objects (Dataframes and SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7nelKMik7bF",
    "outputId": "0a6028ad-9c4e-4ba1-f3ce-3e67d38b8b77"
   },
   "outputs": [],
   "source": [
    "# Read athlete events data as dataframe\n",
    "df_events = spark.read.format('csv')\\\n",
    "            .option('header',True).option('escape','\"')\\\n",
    "            .load('data/athlete_events.csv')\n",
    "\n",
    "# TODO: Read noc regions (countries) data as dataframe\n",
    "df_regions = spark.read.format('csv')\\\n",
    "            .option('header',True)\\\n",
    "            .load('data/noc_regions.csv')\n",
    "\n",
    "# Create Views from Dataframes\n",
    "df_events.createOrReplaceTempView(\"sql_events\")\n",
    "df_regions.createOrReplaceTempView(\"sql_regions\")\n",
    "\n",
    "# View Schema for both dataframes\n",
    "df_events.printSchema()\n",
    "df_regions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u0noHiLVk7bI"
   },
   "source": [
    "### Queries/Anaysis\n",
    "For this part, you will need to implement the Dataframe operations and/or the SQL queries to obtain the reports needed for the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-2\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">2. Lab Task: </strong>Get total number of male athletes per year of the 2000s order by ascending year. <strong>Sample Output:</strong>\n",
    "<pre>\n",
    "+----+------------------+\n",
    "|year|number_of_athletes|\n",
    "+----+------------------+\n",
    "|2000|             XXXXX|\n",
    "|2002|              XXXX|\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-3\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">3. Lab Task: </strong> Get total number of athletes per Olympic event (summer/winter) in the 1990s decade for Australia and New Zealand. <strong>Sample Output:</strong>\n",
    "<pre>\n",
    "+-----------+------+------------------+\n",
    "|    country|season|number_of_athletes|\n",
    "+-----------+------+------------------+\n",
    "|  Australia|Summer|               XXX|\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-4\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>Obtain the minimum, average and maximum height of each country for the Winter Olympics and order by the average value in descending order. <strong>Output should be in the following format:</strong>\n",
    "<pre>\n",
    "+--------------------+----------+------------------+----------+\n",
    "|             country|min_height|        avg_height|max_height|\n",
    "+--------------------+----------+------------------+----------+\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-5\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong> Get the Olympics teams that don't have information of their countries in noc_regions (e.g. Soviet Union since it doesn't exist anymore). <strong>Output should be in the following format:</strong>\n",
    "<pre>\n",
    "+--------------------+---+\n",
    "|                team|noc|\n",
    "+--------------------+---+\n",
    "|               Almaz|URS|\n",
    "|         Australasia|ANZ|\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgba(0,255,0,0.2);padding:10px;border-radius:4px\">\n",
    " <h3>Assignment 1</h3>\n",
    "    Once you are done with the lab tasks, please work on your Assignment 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4Vj16RFIgk"
   },
   "source": [
    "**Congratulations on finishing this activity!**\n",
    "\n",
    "Having practiced today's activities, we're now ready to embark on a trip of the rest of exiciting FIT5202 activities! See you next week!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "FIT5202 - Parallel Aggregation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
