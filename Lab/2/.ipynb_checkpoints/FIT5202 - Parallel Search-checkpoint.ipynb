{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcWYqAxbFIee"
   },
   "source": [
    "# FIT5202 Data processing for Big data\n",
    "\n",
    "##  Activity: Parallel Search\n",
    "\n",
    "For this tutorial, we will focus on parallel search in Big Data. Thus, the following sections will be done:\n",
    "1. Review Data partitioning strategies\n",
    "2. Implement distinct searching functionalities using RDDs: \n",
    "3. Implement distinct searching functionalities using Spark SQL module: you will use the Spark API to use dataframes and Spark SQL to perform the search functionality as in section 1.\n",
    "\n",
    "\n",
    "Also, you will need to  visualise the parallelism on searching in these APIs and RDD implementation. Furthermore, you will need to look at the Query execution plan done by the Spark Optimizer Engine and understand how internally Spark executes or plans a searching function.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [SparkContext and SparkSession](#one)\n",
    "* [Data Partitioning](#two)\n",
    "* [Spark RDDs](#three)\n",
    "    * [Data Partitioning in RDD](#three)\n",
    "        * [Default Partitioning](#default)\n",
    "        * [Hash Partitioning](#hash)\n",
    "        * [Range Partitioning](#range)\n",
    "    * [Parallel Search in RDDs](#parallel-search-rdd)    \n",
    "* [Spark DataFrames](#dataframes)\n",
    "    * [Data Partitioning in DataFrames](#df-partitioning)\n",
    "    * [Parallel Search in DataFrames](#parallel_search_df)    \n",
    "    * [Parallel Search with SparkSQL](#parallel_search_sparksql)       \n",
    "* [Lab Tasks](#lab-task-1)\n",
    "    * [Lab Task 1](#lab-task-1)\n",
    "    * [Lab Task 2](#lab-task-2)\n",
    "    * [Lab Task 3](#lab-task-3)\n",
    "    * [Lab Task 4](#lab-task-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUTW3z5TS-Yt"
   },
   "source": [
    "## SparkContext and SparkSession <a class=\"anchor\" name=\"one\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dk4z3uwkS-Yu"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3b546fb2ecbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import SparkConf class into program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Parallel Search\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "# # Method 2: Getting or instantiating a SparkContext\n",
    "# sc = SparkContext.getOrCreate(spark_conf)\n",
    "# sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yPtiGySS-Yv"
   },
   "source": [
    "## Data Partitioning <a class=\"anchor\" id=\"two\"></a>\n",
    "In this first part of the tutorial, we will do a quick review of a few data partitioning strategies which we will need to know for the rest of the topics in this tutorial.\n",
    "\n",
    "Data partitioning is the fundamental step for parallel search algorithms as parallelism in query and search processing is achieved through data partionining. \n",
    "In this activity, we will consider the following **three** partitioning strategies:\n",
    "#### 1. Round-robin data partitioning ###\n",
    "Round-robin data partitioning is the simplest data partitioning method in which each record in turn is allocated to a processing element (simply processor). Since it distributes the data evenly among all processors, it is also known as \"equal-partitioning\".\n",
    "\n",
    "#### 2. Range data partitioning ###\n",
    "Range data partitioning records based on a given range of the partitioning attribute. For example,the student table is partitioned based on \"Last Name\" based on the alphabetical order (i.e. A ~ Z). \n",
    "\n",
    "#### 3. Hash data partitioning ###\n",
    "Hash data partitioning makes a partition based on a particular attribute using a hash function. The result of a hash function determines the processor where the record will be placed. Thus, all records within a partition have the same hash value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK-02wozS-Yv"
   },
   "source": [
    "## RDD partitioning <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "By default, Spark partitions the data using <strong>Random equal partitioning</strong> unless there are specific transformations that uses a different type of partitioning</strong>\n",
    "In the code below, we have defined two functions to implement custom partitioning using <strong>Range Partitioning</strong> and <strong>Hash Partitioning</strong>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcL71Ly3S-Yw"
   },
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "\n",
    "#A Function to print the data items in each RDD\n",
    "#WARNING: this function is only for demo purpose, it should not be used on large dataset\n",
    "def print_partitions(data):\n",
    "    if isinstance(data, RDD):\n",
    "        numPartitions = data.getNumPartitions()\n",
    "        partitions = data.glom().collect()\n",
    "    else:\n",
    "        numPartitions = data.rdd.getNumPartitions()\n",
    "        partitions = data.rdd.glom().collect()\n",
    "    \n",
    "    print(f\"####### NUMBER OF PARTITIONS: {numPartitions}\")\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "            print(partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK6xUCJrS-Yw"
   },
   "outputs": [],
   "source": [
    "#Sample data used for demonstrating the partitioning\n",
    "list_tutors = [(1,'Aaditya'),(2,'Chinnavit'),(3,'Neha'),(4,'Huashun'),(5,'Mohammad'),\n",
    "                (10,'Peter'),(11,'Paras'),(12, 'Tooba'),(3, 'David'),(18,'Cheng'),(9,'Haqqani')]\n",
    "\n",
    "#Define the number of partitions\n",
    "no_of_partitions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K0D0_b0S-Yw"
   },
   "outputs": [],
   "source": [
    "(1,'Aaditya'),(2,'Chinnavit')\n",
    "(3,'Neha'),(4,'Huashun')\n",
    "(5,'Mohammad'),(10,'Peter')\n",
    "(11,'Paras'),(12, 'Tooba')\n",
    "(3, 'David'),(18,'Cheng')\n",
    "(9,'Haqqani')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8Xh_cTgS-Yx"
   },
   "source": [
    "### Default Partitioning in Spark RDD <a class=\"anchor\" id=\"default\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh2m-aJxS-Yx"
   },
   "outputs": [],
   "source": [
    "# random equal partition\n",
    "rdd = sc.parallelize(list_tutors, no_of_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1pExTkVS-Yx",
    "outputId": "0671a0ae-d060-4dc3-96aa-ad3ee2cca288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:4\n",
      "Partitioner:None\n",
      "####### NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 2 records\n",
      "[(1, 'Aaditya'), (2, 'Chinnavit')]\n",
      "Partition 1: 4 records\n",
      "[(3, 'Neha'), (4, 'Huashun'), (5, 'Mohammad'), (10, 'Peter')]\n",
      "Partition 2: 2 records\n",
      "[(11, 'Paras'), (12, 'Tooba')]\n",
      "Partition 3: 3 records\n",
      "[(3, 'David'), (18, 'Cheng'), (9, 'Haqqani')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions:{}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner:{}\".format(rdd.partitioner))\n",
    "print_partitions(rdd)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSC-sl9MS-Yy"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>How do you think the data is divided across the partitions by default when no partitoner is specified?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaiWieeiS-Yz"
   },
   "source": [
    "### Hash Partitioning in RDD <a class=\"anchor\" id=\"hash\"></a>\n",
    "Hash partitioning uses the formula <code>partition = hash_function() % numPartitions</code> to determine which partition data item falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wTKZPwvS-Yz"
   },
   "outputs": [],
   "source": [
    "#Hash Function to implement Hash Partitioning \n",
    "#Just computes the sum of digits\n",
    "#Example : hash_function(12) produces 3 i.e. 2 + 1\n",
    "def hash_function(key):\n",
    "    total = 0\n",
    "    for digit in str(key):\n",
    "        total += int(digit)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhZGr_8hS-Yz",
    "outputId": "36b1271e-a44b-44f7-b7cf-1c580085e304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 1 records\n",
      "[(4, 'Huashun')]\n",
      "Partition 1: 5 records\n",
      "[(1, 'Aaditya'), (5, 'Mohammad'), (10, 'Peter'), (18, 'Cheng'), (9, 'Haqqani')]\n",
      "Partition 2: 2 records\n",
      "[(2, 'Chinnavit'), (11, 'Paras')]\n",
      "Partition 3: 3 records\n",
      "[(3, 'Neha'), (12, 'Tooba'), (3, 'David')]\n"
     ]
    }
   ],
   "source": [
    "# hash partitioning\n",
    "hash_partitioned_rdd = rdd.partitionBy(no_of_partitions, hash_function)\n",
    "print_partitions(hash_partitioned_rdd)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsIP5pSLS-Y0"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">Note: </strong>Look at how the data is partitioned. For example, Partition 0 has 1 record, [(4, 'Huashun')]. Here is the step-wise breakdown:\n",
    "    <ul>\n",
    "        <li>hash_function(4) = 4</li>\n",
    "        <li>Partition for the key of 4 is determined by <code>hash_function(4)%numPartitions</code> i.e. 4%4=0</li>\n",
    "        <li>Similarly, for (18,'Cheng'), partition is given by <code>hash+function(18)%numPartitions</code> i.e. 9%4=1</li>\n",
    "    </ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj4x0oSgS-Y0"
   },
   "source": [
    "### Range Partitioning in RDD <a class=\"anchor\" id=\"range\"></a>\n",
    "This strategy uses a range to distribute the items to respective partitions when the keys fall within the range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu1a4AQhS-Y0"
   },
   "outputs": [],
   "source": [
    "no_of_partitions=4\n",
    "\n",
    "#Find the size of the elements in RDD\n",
    "chunk_size = len(list_tutors)/no_of_partitions\n",
    "#Define a range of values by key to distribute across partitions\n",
    "#Here for simplicity, we are defining the range i.e. keys from 1-4 to fall in first partition, 5-9 in second partition and so on\n",
    "range_arr=[[1,4],[5,9],[10,14],[15,19]]\n",
    "\n",
    "def range_function(key):\n",
    "    for index,item in enumerate(range_arr):\n",
    "        if key >=item[0] and key <=item[1]:\n",
    "            return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As2D_GhDS-Y0",
    "outputId": "bef71b45-2cf4-4f59-8eda-714cc8731c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### NUMBER OF PARTITIONS: 4\n",
      "Partition 0: 5 records\n",
      "[(1, 'Aaditya'), (2, 'Chinnavit'), (3, 'Neha'), (4, 'Huashun'), (3, 'David')]\n",
      "Partition 1: 2 records\n",
      "[(5, 'Mohammad'), (9, 'Haqqani')]\n",
      "Partition 2: 3 records\n",
      "[(10, 'Peter'), (11, 'Paras'), (12, 'Tooba')]\n",
      "Partition 3: 1 records\n",
      "[(18, 'Cheng')]\n"
     ]
    }
   ],
   "source": [
    "# range partition\n",
    "range_partitioned_rdd = rdd.partitionBy(no_of_partitions, range_function)\n",
    "print_partitions(range_partitioned_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHxVQi5GS-Y1"
   },
   "source": [
    "## Parallel Search using RDDs  <a class=\"anchor\" id=\"parallel-search-rdd\"></a>\n",
    "\n",
    "Now we will implement basic search functionalities and visualise the parallelism embedded in Spark to perform these kind of queries.\n",
    "\n",
    "In this tutorial, you will use a csv dataset **bank.csv**. However, for this tutorial we won't analyse the case study but only perform some search queries with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gXcyINUS-Y1"
   },
   "outputs": [],
   "source": [
    "# Using Spark, we can read and load a csv file\n",
    "# Read csv file and load into an RDD object\n",
    "bank_rdd = sc.textFile('bank.csv')\n",
    "\n",
    "# If you want to specify the number of partitions, you can add the number as a second argument\n",
    "# bank_rdd = sc.textFile('bank.csv', 10)\n",
    "\n",
    "## Exploring the data file, we can see that it contains different types of information\n",
    "## Some useful information is printed below\n",
    "print(f\"Total partitions: {bank_rdd.getNumPartitions()}\")\n",
    "print(f\"Number of lines: {bank_rdd.count()}\")\n",
    "\n",
    "## Each element of the RDD is a line from the file\n",
    "bank_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HlgSrvkS-Y1"
   },
   "source": [
    "### Search in RDDs based on multiple conditions\n",
    "\n",
    "We will focus on only four attributes from the data: age, education, marital and balance for filtering conditions. However, we will display additional information as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTAGTskLS-Y1"
   },
   "outputs": [],
   "source": [
    "# 1. Split each line separated by comma into a list \n",
    "bank_rdd1 = bank_rdd.map(lambda line: line.split(','))\n",
    "# 2. Remove the header\n",
    "header = bank_rdd1.first()\n",
    "bank_rdd1 = bank_rdd1.filter(lambda row: row != header)   #filter out header\n",
    "\n",
    "# Indices for each attribute we will use\n",
    "# Filter: age, education, marital, balance = 0, 3, 2, 5\n",
    "# Display additional: day, month, deposit = 9, 10, 16\n",
    "\n",
    "# 3. Search the records with balance between 1000 and 2000\n",
    "bank_rdd1 = bank_rdd1.filter(lambda x: int(x[5])>1000 and int(x[5])<2000)\n",
    "# 4. Also search the records with primrary or secondary education and age less than 30\n",
    "bank_rdd1 = bank_rdd1.filter(lambda x: x[3] in ['primary','secondary'] and int(x[0])<30)\n",
    "# 5. Also filter with those who are married\n",
    "bank_rdd1 = bank_rdd1.filter(lambda x: x[2]=='married' )\n",
    "# 6. Display the previous attributes plus the information of day, month and deposit\n",
    "bank_rdd1 = bank_rdd1.map(lambda field: (field[0],field[2],field[3],field[5],\n",
    "                                         field[9],field[10],field[16]))\n",
    "# Print how many final records\n",
    "print(bank_rdd1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vg3_M1btS-Y2"
   },
   "outputs": [],
   "source": [
    "# Let's see how the data was divided and the data for each partition\n",
    "numPartitions = bank_rdd1.getNumPartitions()\n",
    "print(f\"Total partitions: {numPartitions}\")\n",
    "\n",
    "# glom(): Return an RDD created by coalescing all elements within each partition into a list\n",
    "# WARNING: glom().collect() only works for a small dataset\n",
    "partitions = bank_rdd1.glom().collect()\n",
    "for index,partition in enumerate(partitions):\n",
    "    print(f'------ Partition {index}:')\n",
    "    for record in partition:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_z0s0r2S-Y2"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>Verify the parallelism in the Spark UI and explore the content. How many jobs have been executed so far?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moRcOHa3S-Y2"
   },
   "source": [
    "### Searching max/min value of an attribute in an RDD\n",
    "This task will aim to find the record in the dataset that contains the highest value for a given attribute. In this case the attribute chosen is \"balance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLaN3rpdS-Y2"
   },
   "outputs": [],
   "source": [
    "# Read csv but now with 4 partitions\n",
    "bank_rdd_4 = sc.textFile('bank.csv',4)\n",
    "\n",
    "# Split and remove the header\n",
    "bank_rdd_4 = bank_rdd_4.map(lambda line: line.split(','))\n",
    "header = bank_rdd_4.first()\n",
    "bank_rdd_4 = bank_rdd_4.filter(lambda row: row != header)   #filter out header\n",
    "\n",
    "# Display the first 3 records\n",
    "bank_rdd_4.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvyYHjeOS-Y2"
   },
   "outputs": [],
   "source": [
    "# Using the RDD function max(), it can be obtained in a single line \n",
    "result_max_balance = bank_rdd_4.max(key=lambda x: x[5]) # Get max by value in index 5 (balance)\n",
    "# Print the record obtain with highest balance\n",
    "print(result_max_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5clozGI8S-Y3"
   },
   "outputs": [],
   "source": [
    "# Get record with balance 10576\n",
    "bank_record = bank_rdd_4.filter(lambda x: x[5]=='10576').collect()\n",
    "print(bank_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNVjM66cS-Y3"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong>Compare the <code>result_max_balance</code> record with the record above <code>(bank_record)</code>. Was the record obtained previously correct i.e. <code>result_max_balance</code>? <b>Explain what happened.</b></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pv9rcrZGS-Y3"
   },
   "outputs": [],
   "source": [
    "result_max_balance = bank_rdd_4.max(key=lambda x: x[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iu2wpH_S-Y3"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-2\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">2. Lab Task: </strong>As you noticed in the previous result, the record returned originally (result_max_balance) was incorrect.\n",
    "    <p><i>Fix the code below that uses the <code>max()</code> function to get the record with the correct maximum balance.</p></i></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrKDskH8S-Y3"
   },
   "source": [
    "#### To learn more about functions in RDDs, you can look into the next 2 sites:\n",
    "1. http://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations\n",
    "1. https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya2G0yqgS-Y3"
   },
   "source": [
    "## DataFrames in Spark <a class=\"anchor\" id=\"dataframes\"></a>\n",
    "A DataFrame is a distributed collection of data organized into named columns. It is equivalent to a table in relational database or a dataframe in R/Python but with richer optimizations under the hood. For more information visit : \n",
    "\n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "### Creating DataFrames\n",
    "SparkSession provides an easy method <code>createDataFrame</code> to create Spark DataFrames. Data can be loaded from csv, json, xml and other sources like local file system or HDFS. More information on : \n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n",
    "\n",
    "To display the schema, i.e. the  structure of the DataFrame, you can use <strong>printSchema()</strong> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b94cQ34rS-Y4"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,'Aaditya','A'),(2,'Chinnavit','C'),(3,'Neha','N'),(4,'Huashun','H'),(5,'Mohammad','M'),\n",
    "                            (10,'Prajwol', 'P'),(1,'Paras','P'),(1, 'Tooba','T'),(3, 'David','D'),(4,'Cheng','C'),(9,'Haqqani','H')],\n",
    "                           ['Id','Name','Initial'])\n",
    "\n",
    "#display the rows of the dataframe\n",
    "df.show(5)\n",
    "#view the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNMnTYXIS-Y4"
   },
   "source": [
    "Another way to create a DataFrame is use the <strong>spark.read.csv</strong> file to load the data from csv to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ccpVmVbS-Y4"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"bank.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3cDmb2zS-Y4"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong>Display first 10 rows of the above dataframe. \n",
    "Try out other dataframe methods:    \n",
    "    <ul>\n",
    "        <li><code>df.columns, df.count()</code></li>\n",
    "        <li><code>df.describe('column_name').show()</code></li>\n",
    "        <li><strong>Selecting:</strong><code>df.select('column_name').show(), df.select('column_name').distinct().show()</code>\n",
    "        </li>\n",
    "        <li><strong>Filtering:</strong><code>df.filter(df.column_name == 123).show()</code></li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG7K3y2IS-Y4"
   },
   "source": [
    "### Partitioning in DataFrames <a class=\"anchor\" id=\"df-partitioning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2DQxGhUS-Y4"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1,'Aaditya','A'),(2,'Chinnavit','C'),(3,'Neha','N'),(4,'Huashun','H'),(5,'Mohammad','M'),\n",
    "                            (10,'Prajwol', 'P'),(1,'Paras','P'),(1, 'Tooba','T'),(3, 'David','D'),(4,'Cheng','C'),(9,'Haqqani','H')],\n",
    "                           ['Id','Name','Initial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iurD9JKYS-Y4"
   },
   "outputs": [],
   "source": [
    "# Round-robin data partitioning\n",
    "df_round = df.repartition(5)\n",
    "# Range data partitioning\n",
    "df_range = df.repartitionByRange(3,\"Name\")\n",
    "# Hash data partitioning\n",
    "column_hash = \"Id\"\n",
    "df_hash = df.repartition(column_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyLhHw2NS-Y5"
   },
   "outputs": [],
   "source": [
    "print_partitions(df_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60dgpUevS-Y5"
   },
   "outputs": [],
   "source": [
    "print_partitions(df_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-eDWxflS-Y5"
   },
   "outputs": [],
   "source": [
    "print_partitions(df_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyJLhO5wS-Y5"
   },
   "outputs": [],
   "source": [
    "# Read csv file and load into a dataframe\n",
    "df = spark.read.csv(\"bank.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suKhXQJHS-Y5"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong> How many partitions the dataframe have?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iSfRC-dS-Y5"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-3\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:##FF5555\">3. Lab Task: </strong> Implement Range and Hash Partitioning techniques for the new dataset and display the partitions. COMPLETE THE CODE BELOW.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g8tgeQwS-Y5"
   },
   "outputs": [],
   "source": [
    "## We can specify how many partitions or what kind of partitioning we want for a dataframe\n",
    "# Round-robin data partitioning\n",
    "df_round = \n",
    "# Range data partitioning\n",
    "df_range = \n",
    "# Hash data partitioning\n",
    "column_hash = \n",
    "df_hash = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O510ExHdS-Y6"
   },
   "outputs": [],
   "source": [
    "## Print the number of partitions for each dataframe\n",
    "print(f\"----- NUMBER OF PARTITIONS df_round: {df_round.rdd.getNumPartitions()}\")\n",
    "print(f\"----- NUMBER OF PARTITIONS df_range: {df_range.rdd.getNumPartitions()}\")\n",
    "print(f\"----- NUMBER OF PARTITIONS df_hash: {df_hash.rdd.getNumPartitions()}\")\n",
    "\n",
    "## Verifying the number of partitions for the dataframe with hash partitioning it would indicate 200.\n",
    "## One important thing is that by default, when the number of partitions are not indicated,\n",
    "## The default number of partitions is 200\n",
    "\n",
    "## However, most of the partitions for df_hash are empty.\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLNM0BNyS-Y6"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong> Complete the above code to show the values for each partition</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAUJ7Ml8S-Y6"
   },
   "outputs": [],
   "source": [
    "## You can verify the partitioning and the query plan when an action is performed with the function explain()\n",
    "# Query plan for df_round\n",
    "df_round.explain()\n",
    "# Query plan for df_range\n",
    "df_range.explain()\n",
    "# Query plan for df_hash\n",
    "df_hash.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLdJB8p2S-Y6"
   },
   "source": [
    "### Parallel Search using Spark Dataframe <a class=\"anchor\" id=\"parallel_search_df\"></a>\n",
    "\n",
    "We will perform the same filtering criteria as in section 1. This time the logic won't be implemented by us but just declare by using the functions of the Spark Dataframe API to perform the same queries. Thus, we should obtain the same results as before.\n",
    "\n",
    "Furthermore, now you will need to see in the Spark UI, the RDD DAG Visualisation and the Execution Plan of the queries performed with the function explain() as we did previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIvf3CLiS-Y6"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-4\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:##FF5555\">4. Lab Task: </strong> Complete the code in the given cell below to implement the given conditions.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ_89LShS-Y6"
   },
   "outputs": [],
   "source": [
    "# Using the Spark Dataframe API we can obtain the dataframe for a csv file\n",
    "# We already created dataframes with different types of partitioning\n",
    "# Choose one of them to work with and perform the queries made in section 1\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "bank_df = df_round\n",
    "\n",
    "## The functions to filter in dataframes are similar to the functions in RDD. Thus, the steps are:\n",
    "# 1. Search the records with balance between 1000 and 2000\n",
    "bank_df = bank_df.filter(col(\"balance\")>1000)\\\n",
    "            .filter(col(\"balance\")<2000)\n",
    "# TODO: \n",
    "# 2. Also in the same dataframe, search the records with primary or secondary education and age less than 30\n",
    "bank_df =\n",
    "\n",
    "# TODO:\n",
    "# 3. Also filter with those who are married\n",
    "bank_df = \n",
    "\n",
    "\n",
    "# TODO:\n",
    "# 4. Display the previous attributes plus the information of day, month and deposit\n",
    "bank_df = \n",
    "\n",
    "\n",
    "# 5. Display the records\n",
    "bank_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OktAZ6PpS-Y6"
   },
   "outputs": [],
   "source": [
    "#### Query and partition information\n",
    "print_partitions(bank_df)\n",
    "#### Execution Plan for query with multiple filter conditions\n",
    "bank_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xZJpq4HS-Y7"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#006DAE\">TODO: </strong> Repeat the same query with different partitioning strategies <strong>(Round-Robin, Range and Hash)</strong> and compare its query execution plan plus its information in the <strong>Spark UI.</strong> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OK7Myn3uS-Y7"
   },
   "source": [
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:##333333\">EXAMPLE: </strong>Obtain also the <code>max/min</code> as you did in RDDs but now using <strong>Spark DataFrame</strong>. Does it return the same value as in section 1? Also, check its execution plan and the information in Spark UI.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aO4WwDStS-Y7"
   },
   "outputs": [],
   "source": [
    "#### SOLUTION:\n",
    "bank_max_balance = df_round.selectExpr(\"int(balance)\").selectExpr(\"max(balance)\").collect()\n",
    "print(bank_max_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlQrkO5kS-Y7"
   },
   "source": [
    "### Parallel Search using SQL language in Spark  <a class=\"anchor\" id=\"parallel_search_sparksql\"></a>\n",
    "#### Spark SQL\n",
    "Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. It enables unmodified Hadoop Hive queries to run up to 100x faster on existing deployments and data. It also provides powerful integration with the rest of the Spark ecosystem (e.g., integrating SQL query processing with machine learning). <a href=\"https://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data\">[Read More]</a>.\n",
    "\n",
    "A view can be created from a dataframe in order to use SQL queries to search data. In this section, you will use SQL statements to perform search queries in the views that will be registered from the dataframes we created in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j2jW4M9S-Y7"
   },
   "outputs": [],
   "source": [
    "# register the original DataFrame as a temp view so that we can query it using SQL\n",
    "df.createOrReplaceTempView(\"df_sql\")\n",
    "filter_sql = spark.sql('''\n",
    "  SELECT age,education,balance,day,month,deposit\n",
    "  FROM df_sql\n",
    "  WHERE balance between 1000 and 2000\n",
    "  AND education in ('secondary','primary')\n",
    "  AND age < 30\n",
    "  AND marital = 'married'\n",
    "''')\n",
    "# filter_sql.explain()\n",
    "filter_sql.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrqZl60gS-Y7"
   },
   "source": [
    "<a class=\"anchor\" id=\"lab-task-6\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#333333\">EXAMPLE: </strong>Obtain also the <code>max/min</code> as you did in RDDs and DataFrames, but now using <strong>Spark SQL</strong>. Does it return the same value as in previous cases? Also, check its execution plan and the information in Spark UI.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYHe6whMS-Y7"
   },
   "outputs": [],
   "source": [
    "#### SOLUTION:\n",
    "max_sql = spark.sql('''\n",
    "  SELECT MAX(CAST(balance AS INT)) as max_balance\n",
    "  FROM df_sql\n",
    "''')\n",
    "# Check the result obtained\n",
    "max_sql.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq4Vj16RFIgk"
   },
   "source": [
    "**Congratulations on finishing this activity!**\n",
    "\n",
    "Having practiced today's activities, we're now ready to embark on a trip of the rest of exiciting FIT5202 activities! See you next week!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "yVWYWwzMFIfR",
    "K2QtBnKgFIfa",
    "kL88Q46yFIfh",
    "48_7UVktFIgD",
    "dtN67ydpFIgF",
    "cSs0qd02FIgI"
   ],
   "name": "Session2_Parallel Search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
