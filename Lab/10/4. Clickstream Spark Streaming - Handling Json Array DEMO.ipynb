{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Json Array streaming from Kafka\n",
    "Here, we have some instructions on how to handle array of json. Make sure your <b>Kafka Producer</b> is publishing an Array of objects.\n",
    "\n",
    "### Step 1 : Initialize Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Clickstream Analysis in Spark\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Read Stream from the Kafka Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"clickstream\"\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Schema and Parsing the data\n",
    "Here, since we are receiving data as an <strong>array</strong> compared with a single object in the previous example, we need to use <code>ArrayType</code> while defining our schema.\n",
    "\n",
    "Before Running this, make sure that your that you created in <strong>Week 9 , LT1-Producer.ipynb</strong> is producing data in the following or similar format:\n",
    "<code>\n",
    "[{'Clicks': 0, 'Impressions': 3, 'ts': 1603072527}, {'Clicks': 0, 'Impressions': 3, 'ts': 1603072527}, {'Clicks': 0, 'Impressions': 3, 'ts': 1603072527}, {'Clicks': 0, 'Impressions': 3, 'ts': 1603072527}, {'Clicks': 0, 'Impressions': 11, 'ts': 1603072527}, {'Clicks': 1, 'Impressions': 11, 'ts': 1603072527}]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the schema for the structured datastream received\n",
    "schema = ArrayType(StructType([    \n",
    "    StructField('Clicks', IntegerType(), True), \n",
    "    StructField('Impressions', IntegerType(), True),\n",
    "    StructField('ts', TimestampType(), True)            \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- parsed_value: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Clicks: integer (nullable = true)\n",
      " |    |    |-- Impressions: integer (nullable = true)\n",
      " |    |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice the schema above, the <strong>Columns</strong> are nested. We can use the <code>explode</code> function to flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- unnested_value: struct (nullable = true)\n",
      " |    |-- Clicks: integer (nullable = true)\n",
      " |    |-- Impressions: integer (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the <strong>.explode()</strong>, the schema looks normal again, we can now proceed with the rest of the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = df.select(\n",
    "                    F.col(\"unnested_value.Clicks\").alias(\"Clicks\"),\n",
    "                    F.col(\"unnested_value.Impressions\").alias(\"Impressions\"),\n",
    "                    F.col(\"unnested_value.ts\").alias(\"ts\")\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
