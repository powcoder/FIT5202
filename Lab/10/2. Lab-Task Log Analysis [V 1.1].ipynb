{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Include Libraries and Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries and initialize Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use-Case : Tracking Server Access Log <a class=\"anchor\" name=\"use-case\"></a>\n",
    "For this case, a server is going to continuously send a records of a host who is trying to access some endpoint (url) from the web server. This data will be send from a kafka producer (<code>2.Lab-Task-KafkaProducer.ipynb</code>) which is reading the data from a txt file in the dataset provided (<code>logs/access_log.txt</code>).\n",
    "\n",
    "Each line contains some valuable information such as:\n",
    "\n",
    "1. Host\n",
    "2. Timestamp\n",
    "3. HTTP method\n",
    "4. URL endpoint\n",
    "5. Status code\n",
    "6. Protocol\n",
    "7. Content Size\n",
    "\n",
    "The goal here is to perform some real time queries from this stream of data and be able to output the results in multiple ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Load Kafka Stream \n",
    "Use the <code>readStream</code> to load data from the Kafka Producer <strong>2.Lab-Task-KafkaProducer.ipynb</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-1\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">1. Lab Task: </strong> \n",
    "    Write the code below to readStream from the the producer into <code>df_urls</code> dataframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the logs data stream for new log data\n",
    "topic = \"w10_access_log\"\n",
    "df_urls = #WRITE THE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation <a class=\"anchor\" name=\"data-prep\"></a>\n",
    "We need to convert the data from the message in order to perform some queries. The steps to parse the data are:\n",
    "\n",
    "1. Get message as a string from <code>value</code> which is binary.\n",
    "2. Implement some regular expressions to capture specific fields in the message which is a line from the access log.\n",
    "3. Extract the values using the regular expressions to create the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get value of the kafka message\n",
    "log_lines = df_urls.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse out the common log format to a DataFrame\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "\n",
    "df_logs = log_lines.select(regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         regexp_extract('value', statusExp, 1).cast('integer').alias('status'))\n",
    "\n",
    "df_logs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Streaming Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-2\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">2. Lab Task: </strong> \n",
    "    Write a DataFrame query to filter out those requests that were not successful using <code>status !=200</code> filter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DF that filters those requests that were not successful (status != 200)\n",
    "unsucess_df = #WRITE THE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-3\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">3. Lab Task: </strong> \n",
    "    Write a DataFrame query count the number of requests by access status code\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DF that keeps a running count of every access by status code\n",
    "status_count_df = #WRITE THE CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output sink <a class=\"anchor\" name=\"output-sink\"></a>\n",
    "Before starting this section, run the kafka producer (<code>2.Lab-Task-KafkaProducer.ipynb</code>) that will send the data from the access log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to show values received from input dataframe\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display stream output in notebook <a class=\"anchor\" name=\"foreachBatch\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output of status_count_df in output cell using the foreach_batch_function\n",
    "# Control the amount of times output is displayed with trigger function\n",
    "query1 = status_count_df.writeStream.outputMode(\"complete\")\\\n",
    "        .foreachBatch(foreach_batch_function)\\\n",
    "        .trigger(processingTime='5 seconds')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"lab-task-4\"></a>\n",
    "<div style=\"background:rgba(0,109,174,0.2);padding:10px;border-radius:4px\"><strong style=\"color:#FF5555\">4. Lab Task: </strong> \n",
    "    Write the stream output to the <strong>memory sink</strong> and display the result using <strong>spark SQL</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE THE CODE HERE TO WRITE OUTPUT TO MEMORY SINK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE THE CODE HERE TO QUERY THE TABLE FROM MEMORY SINK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
